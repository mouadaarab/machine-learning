#!/usr/bin/env python3
"""
Script pour cr√©er et sauvegarder des mod√®les XGBoost 
entra√Æn√©s sur les datasets Iris et California Housing.

Ce script cr√©e deux mod√®les XGBoost :
1. XGBClassifier pour la classification des esp√®ces d'iris
2. XGBRegressor pour la pr√©diction des prix immobiliers californiens
"""

import os
import joblib
import numpy as np
import pandas as pd
from sklearn.datasets import load_iris, fetch_california_housing
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import (
    classification_report, confusion_matrix, accuracy_score,
    mean_squared_error, r2_score
)

try:
    import xgboost as xgb
    from xgboost import XGBClassifier, XGBRegressor
except ImportError:
    print("‚ùå XGBoost n'est pas install√©. Installation en cours...")
    import subprocess
    subprocess.check_call(['pip', 'install', 'xgboost'])
    import xgboost as xgb
    from xgboost import XGBClassifier, XGBRegressor


def create_xgboost_iris_model():
    """
    Cr√©e et sauvegarde un mod√®le XGBoost pour la classification des esp√®ces d'iris.
    
    Returns:
        XGBClassifier: Le mod√®le entra√Æn√©
        dict: Informations sur les performances du mod√®le
    """
    print("\n" + "="*60)
    print("üå∏ Cr√©ation du mod√®le XGBoost pour la classification Iris...")
    
    # Charger le dataset Iris
    iris = load_iris()
    X = iris.data
    y = iris.target
    feature_names = iris.feature_names
    target_names = iris.target_names
    
    print(f"üìä Dataset charg√© avec succ√®s:")
    print(f"   - Nombre d'√©chantillons: {X.shape[0]}")
    print(f"   - Nombre de features: {X.shape[1]}")
    print(f"   - Classes: {target_names}")
    
    # Diviser les donn√©es
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )
    
    print(f"‚úÇÔ∏è  Division des donn√©es:")
    print(f"   - Entra√Ænement: {X_train.shape[0]} √©chantillons")
    print(f"   - Test: {X_test.shape[0]} √©chantillons")
    
    # Cr√©er et entra√Æner le mod√®le XGBoost
    print("üöÄ Entra√Ænement du mod√®le XGBoost Classifier...")
    
    xgb_classifier = XGBClassifier(
        n_estimators=100,
        max_depth=6,
        learning_rate=0.1,
        subsample=0.8,
        colsample_bytree=0.8,
        random_state=42,
        eval_metric='mlogloss'
    )
    
    xgb_classifier.fit(X_train, y_train)
    
    # √âvaluer le mod√®le
    print("üìà √âvaluation du mod√®le...")
    
    # Pr√©dictions sur l'ensemble de test
    y_pred = xgb_classifier.predict(X_test)
    
    # Calculer l'accuracy
    accuracy = accuracy_score(y_test, y_pred)
    print(f"üéØ Accuracy sur l'ensemble de test: {accuracy:.4f} ({accuracy*100:.2f}%)")
    
    # Validation crois√©e
    cv_scores = cross_val_score(xgb_classifier, X, y, cv=5)
    print(f"üîÑ Validation crois√©e (5-fold):")
    print(f"   - Score moyen: {cv_scores.mean():.4f} (¬±{cv_scores.std()*2:.4f})")
    
    # Rapport de classification d√©taill√©
    print("\nüìã Rapport de classification:")
    print(classification_report(y_test, y_pred, target_names=target_names))
    
    # Matrice de confusion
    print("üî¢ Matrice de confusion:")
    cm = confusion_matrix(y_test, y_pred)
    print(cm)
    
    # Importance des features
    print("\nüèÜ Importance des features:")
    feature_importance = pd.DataFrame({
        'feature': feature_names,
        'importance': xgb_classifier.feature_importances_
    }).sort_values('importance', ascending=False)
    
    for idx, row in feature_importance.iterrows():
        print(f"   {row['feature']}: {row['importance']:.4f}")
    
    # Sauvegarder le mod√®le
    models_dir = 'models_ai'
    if not os.path.exists(models_dir):
        os.makedirs(models_dir)
    
    model_path = os.path.join(models_dir, 'xgboost_iris_model.pkl')
    joblib.dump(xgb_classifier, model_path)
    
    # Informations sur le mod√®le
    model_info = {
        'model_type': 'XGBClassifier',
        'features': list(feature_names),
        'target_names': list(target_names),
        'n_estimators': 100,
        'max_depth': 6,
        'learning_rate': 0.1,
        'accuracy': accuracy,
        'cv_mean_score': cv_scores.mean(),
        'cv_std_score': cv_scores.std(),
        'feature_importance': feature_importance.to_dict('records'),
        'confusion_matrix': cm.tolist()
    }
    
    info_path = os.path.join(models_dir, 'xgboost_iris_model_info.pkl')
    joblib.dump(model_info, info_path)
    
    print(f"\nüíæ Mod√®le sauvegard√©:")
    print(f"   - Mod√®le: {model_path}")
    print(f"   - Informations: {info_path}")
    
    return xgb_classifier, model_info


def create_xgboost_housing_model():
    """
    Cr√©e et sauvegarde un mod√®le XGBoost pour la pr√©diction des prix immobiliers californiens.
    
    Returns:
        XGBRegressor: Le mod√®le entra√Æn√©
        dict: Informations sur les performances du mod√®le
    """
    print("\n" + "="*60)
    print("üè† Cr√©ation du mod√®le XGBoost pour la r√©gression California Housing...")
    
    # Charger le dataset California Housing
    california_housing = fetch_california_housing()
    X = california_housing.data
    y = california_housing.target
    feature_names = california_housing.feature_names
    
    print(f"üìä Dataset charg√© avec succ√®s:")
    print(f"   - Nombre d'√©chantillons: {X.shape[0]}")
    print(f"   - Nombre de features: {X.shape[1]}")
    print(f"   - Features: {feature_names}")
    print(f"   - Target: Prix des maisons (en centaines de milliers de dollars)")
    
    # Diviser les donn√©es
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42
    )
    
    print(f"‚úÇÔ∏è  Division des donn√©es:")
    print(f"   - Entra√Ænement: {X_train.shape[0]} √©chantillons")
    print(f"   - Test: {X_test.shape[0]} √©chantillons")
    
    # Cr√©er et entra√Æner le mod√®le XGBoost
    print("üöÄ Entra√Ænement du mod√®le XGBoost Regressor...")
    
    xgb_regressor = XGBRegressor(
        n_estimators=100,
        max_depth=6,
        learning_rate=0.1,
        subsample=0.8,
        colsample_bytree=0.8,
        random_state=42,
        eval_metric='rmse'
    )
    
    xgb_regressor.fit(X_train, y_train)
    
    # √âvaluer le mod√®le
    print("üìà √âvaluation du mod√®le...")
    
    # Pr√©dictions sur l'ensemble de test
    y_pred = xgb_regressor.predict(X_test)
    
    # Calculer les m√©triques
    mse = mean_squared_error(y_test, y_pred)
    rmse = np.sqrt(mse)
    r2 = r2_score(y_test, y_pred)
    
    print(f"üìä Performance du mod√®le:")
    print(f"   - Mean Squared Error: {mse:.4f}")
    print(f"   - Root Mean Squared Error: {rmse:.4f}")
    print(f"   - R¬≤ Score: {r2:.4f}")
    
    # Validation crois√©e pour la r√©gression
    cv_scores = cross_val_score(xgb_regressor, X, y, cv=5, scoring='r2')
    print(f"üîÑ Validation crois√©e R¬≤ (5-fold):")
    print(f"   - Score moyen: {cv_scores.mean():.4f} (¬±{cv_scores.std()*2:.4f})")
    
    # Importance des features
    print("\nüèÜ Importance des features:")
    feature_importance = pd.DataFrame({
        'feature': feature_names,
        'importance': xgb_regressor.feature_importances_
    }).sort_values('importance', ascending=False)
    
    for idx, row in feature_importance.iterrows():
        print(f"   {row['feature']}: {row['importance']:.4f}")
    
    # Sauvegarder le mod√®le
    models_dir = 'models_ai'
    if not os.path.exists(models_dir):
        os.makedirs(models_dir)
    
    model_path = os.path.join(models_dir, 'xgboost_housing_model.pkl')
    joblib.dump(xgb_regressor, model_path)
    
    # Informations sur le mod√®le
    model_info = {
        'model_type': 'XGBRegressor',
        'features': list(feature_names),
        'n_estimators': 100,
        'max_depth': 6,
        'learning_rate': 0.1,
        'mse': mse,
        'rmse': rmse,
        'r2_score': r2,
        'cv_mean_score': cv_scores.mean(),
        'cv_std_score': cv_scores.std(),
        'feature_importance': feature_importance.to_dict('records')
    }
    
    info_path = os.path.join(models_dir, 'xgboost_housing_model_info.pkl')
    joblib.dump(model_info, info_path)
    
    print(f"\nüíæ Mod√®le sauvegard√©:")
    print(f"   - Mod√®le: {model_path}")
    print(f"   - Informations: {info_path}")
    
    return xgb_regressor, model_info


def test_iris_model():
    """
    Test rapide du mod√®le Iris avec des exemples.
    """
    print(f"\nüß™ Test du mod√®le XGBoost Iris...")
    
    model_path = os.path.join('models_ai', 'xgboost_iris_model.pkl')
    
    if not os.path.exists(model_path):
        print(f"‚ùå Mod√®le non trouv√©: {model_path}")
        return
    
    # Charger le mod√®le
    model = joblib.load(model_path)
    
    # Exemples de test (mesures typiques pour chaque classe)
    test_samples = [
        [5.1, 3.5, 1.4, 0.2],  # Setosa typique
        [7.0, 3.2, 4.7, 1.4],  # Versicolor typique
        [6.3, 3.3, 6.0, 2.5]   # Virginica typique
    ]
    
    sample_names = ['Setosa typique', 'Versicolor typique', 'Virginica typique']
    class_names = ['Setosa', 'Versicolor', 'Virginica']
    
    print("üîÆ Pr√©dictions sur des √©chantillons tests:")
    for i, (sample, name) in enumerate(zip(test_samples, sample_names)):
        prediction = model.predict([sample])[0]
        probability = model.predict_proba([sample])[0]
        
        print(f"   {name}:")
        print(f"      Features: {sample}")
        print(f"      Pr√©diction: {class_names[prediction]}")
        print(f"      Probabilit√©s: {dict(zip(class_names, probability.round(4)))}")


def test_housing_model():
    """
    Test rapide du mod√®le California Housing avec des exemples.
    """
    print(f"\nüß™ Test du mod√®le XGBoost California Housing...")
    
    model_path = os.path.join('models_ai', 'xgboost_housing_model.pkl')
    
    if not os.path.exists(model_path):
        print(f"‚ùå Mod√®le non trouv√©: {model_path}")
        return
    
    # Charger le mod√®le
    model = joblib.load(model_path)
    
    # Exemples de test (valeurs moyennes typiques)
    test_samples = [
        [8.3252, 41.0, 6.984127, 1.023810, 322.0, 2.555556, 37.88, -122.23],  # San Francisco typique
        [5.6431, 25.0, 4.192308, 1.022222, 1392.0, 3.877778, 36.06, -119.01], # Central Valley typique
        [3.2596, 33.0, 3.257576, 1.017241, 2599.0, 4.466667, 34.03, -118.38]  # Los Angeles typique
    ]
    
    sample_names = ['San Francisco', 'Central Valley', 'Los Angeles']
    feature_names = [
        'MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 
        'Population', 'AveOccup', 'Latitude', 'Longitude'
    ]
    
    print("üîÆ Pr√©dictions sur des √©chantillons tests:")
    for i, (sample, name) in enumerate(zip(test_samples, sample_names)):
        prediction = model.predict([sample])[0]
        
        print(f"   {name}:")
        print(f"      Pr√©diction: ${prediction*100:.0f}k")
        print(f"      Features: {dict(zip(feature_names, sample))}")


def main():
    """
    Fonction principale pour cr√©er les mod√®les XGBoost.
    """
    print("üöÄ Cr√©ation des mod√®les XGBoost")
    print("=" * 60)
    
    try:
        # Cr√©er le mod√®le Iris
        iris_model, iris_info = create_xgboost_iris_model()
        
        # Cr√©er le mod√®le California Housing
        housing_model, housing_info = create_xgboost_housing_model()
        
        # Tester les mod√®les
        test_iris_model()
        test_housing_model()
        
        print("\nüéâ Tous les mod√®les XGBoost ont √©t√© cr√©√©s avec succ√®s!")
        print("Les mod√®les sont pr√™ts √† √™tre utilis√©s dans votre application Django.")
        
        return {
            'iris': {'model': iris_model, 'info': iris_info},
            'housing': {'model': housing_model, 'info': housing_info}
        }
        
    except Exception as e:
        print(f"‚ùå Erreur lors de la cr√©ation des mod√®les: {str(e)}")
        raise


if __name__ == "__main__":
    models = main()
